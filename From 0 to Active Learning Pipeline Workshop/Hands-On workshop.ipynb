{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Dataloop.ai - a platform workshop\n"
      ],
      "metadata": {
        "id": "GVtyqBasiT3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This document will provide instructions on how to get started with dataloop platform and tthe hands on experience on most on the platform components.\n",
        "\n",
        "\n",
        "You will cover a full active learning pipeline by the end of the workshop .\n",
        "\n"
      ],
      "metadata": {
        "id": "HA5IQ2Hjia1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Set up your Data environment:\n",
        "In this step we will create a dataset that will be used afterwards to manage the data and automate the pipeline,along with creating a recipy that will define the annotation prossess and help in searcing / filtering  the annotated data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qe7gSWeSidlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1.1 Create a Dataset :\n",
        "From the **Data page**, click on **'create dataset'**, give it a name and select the **'file system'** storage option."
      ],
      "metadata": {
        "id": "Q9BZLwdP-31_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1.2 Edit the Ontology :\n",
        "From the Ontology page\n",
        " double click and open the previously created dataset ontology .\n",
        "\n",
        "1.   Double click and open the previously created dataset ontology.\n",
        "2.   In labels and attributes, click the import labels icon, and import the **labels.txt** file from the attached files.\n",
        "3. In instructions click on the **Annotation Instructions** and upload the instructions.pdf in files for the annotation team.\n",
        "4.In **Annotations Verification**, upload the ? to upload or not?\n",
        "\n",
        "\n",
        ">NOTE: You can create multiple ontologies and switch between them for the dataset based on the mission.\n"
      ],
      "metadata": {
        "id": "5yiJhU5t-1V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 2. Build basic pipeline flow:\n",
        "In this step we will create basic pipeline flow, that contains a dataset, small script and an annotation task.\n"
      ],
      "metadata": {
        "id": "VNc9_7Sv3O03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.1 Create a pipeline :\n",
        "From the **Pipelines page**, click on **create pipeline**, then **Start from scratch** ,give it a name and create.\n"
      ],
      "metadata": {
        "id": "7DLW5lfF-vvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2 Edit the pipeline :\n",
        "From the elements panel :\n",
        "\n",
        "1.   Drag and drop the **Data element - Dataset**.\n",
        "2.   Click on the added element and edit it's info panel on the right side, by selecting the previusly created dataset,and turning the trigger toggle on for **once an item is created**.\n",
        "3. Drag and drop the **Automation element - Code** and connect the previous element with it.\n",
        "4.Click on the added element and edit it's info panel :\n",
        "  * Node Name as - **Random Box**\n",
        "  * Application Name as - **YourName-add-random**\n",
        "  * Source Code Type as - **Code Editor**\n",
        "  * Open Code Editor and copy paste the following code and save it.\n",
        "5. Drag and drop the **Automation element - FAAS ** and connect the previous element with it.\n",
        "6.On the info panel,search for the **Flip_item** pre-added function,that will flip the item before passing it to the labeling task.\n",
        "6.Drag and drop the **Workflow element - Labeling** and connect the previous element with it..\n",
        "7.From the info panel,click on **Create Task** and fill out the task requirements then create the task .\n",
        "\n",
        ">NOTE:The Dataset selected in the dataset node must match the dataset configured in the labeling task."
      ],
      "metadata": {
        "id": "qF0Y79iU-r5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dtlpy as dl\n",
        "\n",
        "class ServiceRunner:\n",
        "\n",
        "    def run(self, item):\n",
        "        builder = item.annotations.builder()\n",
        "        builder.add(annotation_definition=dl.Box(top=10,left=10,bottom=100, right=100,label='person'))\n",
        "        item.annotations.upload(builder)\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "Xqa3x4cc6HYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.3 Run and execute the pipeline :\n",
        "1. Click on **Start Pipeline** to start the pipeline.\n",
        "2. Open the Dataset that you have created previously from the **Data Page** and upload data from the **Sample_Data** attached file.\n",
        "3. Track the items cycles and untill they get to the labeling task .\n",
        "4. Go the the Labeling page and open your rtask to complete the cycle.\n",
        "\n",
        ">NOTE:Uploading the items will trigger the pipeline to execute and create a cycle for each one of them, once the item is done in an element, it will be automatically moved to the next one."
      ],
      "metadata": {
        "id": "jptg4uW-6dox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Build an Active learning pipeline:\n",
        " The Dataloop's Active Learning Pipeline (ALP) is a powerful and customizable feature designed to automate and streamline the iterative model training process on unstructured data.\n",
        " The ability to a\n",
        " utomate the iterative process of fine-tuning your model, covering labeling, model creation, training, evaluation, and deployment of the best version.\n"
      ],
      "metadata": {
        "id": "KVrDz3RO_AmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.1 Create a model :\n",
        "\n",
        "1.   From the **Models page**, go to the **'AI Library'** and view the Public plications to see the several pre-installed and publicly available model architectures, which can be installed in your project and trained with your data to achieve pre-annotations of your labels.\n",
        "2.   Click on the create model icon next to the **Yolov8 model** .\n",
        "3.   Edit the Model name to **YourName-cloned-yolov8**.\n",
        "4.   In **Data & Instructions** turn off the finetune option, since we are going to edit the training fields in the pipeline.\n",
        "5.   Click on **Create Model**.\n",
        "\n",
        ">NOTE:Dataloop icon next to the base models list, represents a public trained model from our AI library that you can easly clone and start from it."
      ],
      "metadata": {
        "id": "ti6cf_3B_b91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2 Create the Active learning pipeline from a template:\n",
        "\n",
        "1.   From the **Pipelines Page** click on **Create Pipeline** and then select the option to **Use a Template**.\n",
        "2.   Search for the **Active Learning Template**,click on it and click on install to install it's application.\n",
        "3.   Click on **Create pipeline** then go to the **Actions menu** to rename it\n",
        " as **YourName-ALP**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3xkSaueloti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.3 Edit the Active learning pipeline:\n",
        "The flow of Dataloop's active learning pipeline is divided into two flows:\n",
        "\n",
        "1. Ground Truth Enrichment: Collecting train Data into the Ground Truth - The upper flow\n",
        "2. Training a new version of your model based on the collected data - The lower flow\n",
        "\n",
        "> Note : For more information about the pipeline nodes, see the [Pipeline Nodes Document](https://dataloop.ai/docs/pipeline-nodes).\n",
        "\n"
      ],
      "metadata": {
        "id": "kdPaCdy5ubp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.3.1  Upper Flow: Collection of Ground-Truth Data:\n",
        "\n",
        "\n",
        "1. Pipeline Trigger:  \n",
        "By default, an **item_created** event trigger is set on the first node ('Dataset Node') to trigger the ALP on every item creation (following data upload/sync) in the dataset you set on the Dataset Node. This trigger can be modified so you can select different events or add a DQL filter to the trigger.\n",
        "\n",
        "2. Dataset Node:   \n",
        "Use a Dataset node as the starting point of your pipeline to ensure it only triggers events in that specific - **Select the dataset you've created in step 1**.\n",
        "> Note :The Dataset selected in this node must match the dataset configured in the Annotation workflow task nodes after the predict node.\n",
        "\n",
        "3. Predict Node:  \n",
        "The active learning pipeline begins with the **Predict node**, where a pre-trained model generates annotations on unlabeled data, using the model set under **Best model variable**.\n",
        "\n",
        " *   Go to the **Pipeline Variables** and click on **Manage Variables**.\n",
        " *   Edit the **Best Model Variable** and add your **Model Id** as the value.\n",
        " *   Click the **v icon** to add it and **Save Changes**.\n",
        "\n",
        "\n",
        "4. Human in The Loop (HITL):   \n",
        "Once pre-annotation using your model is done, **data items move directly into human annotation workflow tasks according to your needs**.  \n",
        " The default ALP consists of a labeling task and a QA (Review) task to correct the model's pre-annotations. Customize the workflow section according to your needs and fully set up the task.\n",
        "\n",
        "5. Split Data into Ground Truth Subsets - ML Data Split Node :   \n",
        "It randomly splits the annotated data into three subset groups based on the given distribution percentage.   \n",
        "By default, the distribution is: **80-10-10** for Train-Validation-Test subsets.  \n",
        "Metadata tags will be added to the items' metadata under metadata.system.tags\n",
        " > For example:   \n",
        " {\n",
        "  \"metadata\": {\n",
        "    \"system\": {\n",
        "      \"tags\": {\n",
        "      \"train\": true\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}  \n",
        "These metadata tags to be later easily found and used as DQL filter for the train and evaluation.\n",
        "\n",
        "6. Dataset Node - Ground Truth:  \n",
        " Select the Ground Truth dataset to store the new data in.\n",
        " The data will be cloned from the dataset selected in the first pipeline node.\n",
        " > Note: If you want to use the same dataset as you set in the pipeline starting point (the first node), then you can either remove the current Dataset node or keep it there; it won't affect the data."
      ],
      "metadata": {
        "id": "yLu3tzR0I8RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.3.2  Lower Flow: Create, Train, Deploy New Model Version:\n",
        "\n",
        "\n",
        "1. Have_new_items FAAS -   \n",
        " *   Drag and drop the **Automation element - FAAS** and connect the create new model element.\n",
        " *   From the info panel,search and select the **Have_new_items** pre-added function,that will wait every x items and execute the training flow.\n",
        " *   Define the item_count_threshold variable from the info panel .\n",
        "\n",
        "2. Create a New Model Node:   \n",
        "This node clones a base model using the variables set on the node inputs (dataset, training and validation subsets, and model configurations). This node outputs a model that is prepared for training.  \n",
        " The name of the new model entity is taken from the **New Model Name text box** in the node's panel.  \n",
        " > Note: If a model already exists with the same name, an incremental number will be automatically added as a suffix.\n",
        "\n",
        "3. Train Model Node:   \n",
        "The Train Model node executes the training process for the newly created model version.  \n",
        "The model will be trained over the Ground-Truth dataset using the train and validation subsets that were saved on the model during its creation.\n",
        "\n",
        "4. Evaluate Node - Base Model:   \n",
        "The Evaluate Model node creates evaluation for the base model to make sure its evaluation is up-to-date, and based on the entire test subset of the Ground-Truth dataset (as defined in the variables).   \n",
        "The evaluation will be used later for comparison with the newly created model ('Best model' variable).\n",
        "\n",
        "5. Compare Models Node :  \n",
        "The Compare Models node compares two models: a previously trained model and a newly trained model.\n",
        "\n",
        "\n",
        "6. Update Variable Node :   \n",
        "If the winning model is the new model (based on the comparison config), the Update Variable node will automatically deploy the model and update the **Best model** variable.  \n",
        "Your pipeline will start using the new model immediately across the pipeline - every node that uses this variable (predict, create model)."
      ],
      "metadata": {
        "id": "pg9PEbmIJHIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Executing the Pipeline\n",
        "Once you configure your Active Learning Pipeline:  \n",
        "\n",
        "1.   Click **Start Pipeline** to activate it.  \n",
        "2.   Trigger the pipeline :\n",
        " *   Automatically : By uploading items from the **Sample_Data** file to the triggered dataset.\n",
        " *   Manually trigger the pipeline over existing data by going to the Browser of the triggered dataset,filter the required data and click Dataset Actions > Run with pipeline > Select your pipeline.\n",
        "3. Track the items in the pipeline and start working !\n",
        "\n"
      ],
      "metadata": {
        "id": "Lp7PAww1c9Up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful Links:\n",
        "[Active Learning Pipeline Document](https://dataloop.ai/docs/active-learning-pipeline).\n",
        "[Model Training Document](https://dataloop.ai/docs/model-training).\n",
        "[Pipeline Nodes Document](https://dataloop.ai/docs/pipeline-nodes).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChFj4bGigz4g"
      }
    }
  ]
}